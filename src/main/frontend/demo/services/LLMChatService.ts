import type { Subscription } from '@vaadin/hilla-frontend';
import type Message from 'Frontend/generated/com/vaadin/demo/component/messages/LLMClient/Message';

// This content is used by message-list-markdown.tsx and message-list-markdown.ts.
// See LLMClient.java for the equivalent used by MessageListMarkdown.java.
const answerMarkdown = `
_(Please note: This is just a placeholder message and not generated by a real language model.)_

I can help you with:

1. **Answering questions** ‚Äì from quick facts to in-depth explanations.  
2. **Explaining concepts** ‚Äì breaking down complex ideas into clear, step-by-step logic.  
3. **Brainstorming & creativity** ‚Äì generating outlines, stories, code snippets, or design ideas.  
4. **Guidance & troubleshooting** ‚Äì walking you through processes or helping debug issues.  

---

### How to get the most out of me üõ†Ô∏è

| Step | What to do | Why it matters |
|------|------------|----------------|
| 1Ô∏è‚É£ | **State your goal clearly.** | A precise prompt yields a precise answer. |
| 2Ô∏è‚É£ | **Add constraints or context.** <br>*(e.g., audience, length, tone)* | Tailors the response to your needs. |
| 3Ô∏è‚É£ | **Ask follow-ups.** | We can iterate until you're satisfied. |

Need anything else? Just let me know, and I'll jump right in! ‚ú®`;

class LLMChatService {
  async getHistory(_chatId: string): Promise<Message[]> {
    // Simulate fetching chat history
    return Promise.resolve([
      {
        text: 'Hello! How can I assist you today?',
        assistant: true,
      },
    ]);
  }

  stream(_chatId: string, _userMessage: string): Subscription<string> {
    let onNextCallback: ((token: string) => void) | null;
    let onCompleteCallback: (() => void) | null;

    // Create subscription interface
    const subscription = {
      onNext: (callback: (token: string) => void) => {
        onNextCallback = callback;

        // Simulate token-by-token streaming with a small delay
        const tokens = answerMarkdown.split(' ');

        setTimeout(async () => {
          let tokenIndex = 0;
          while (tokenIndex < tokens.length) {
            const token = `${tokens[tokenIndex]} `;
            tokenIndex += 1;
            onNextCallback!(token);
            // eslint-disable-next-line no-await-in-loop
            await new Promise((resolve) => {
              setTimeout(resolve, 100);
            });
          }
          if (onCompleteCallback) {
            onCompleteCallback();
          }
        }, 1000);

        return subscription;
      },
      onComplete: (callback: () => void) => {
        onCompleteCallback = callback;
        return subscription;
      },
    };

    // @ts-expect-error partially implemented
    return subscription;
  }
}

export default new LLMChatService();
