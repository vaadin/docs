---
title: Technical-Setup
page-title: Technical-Setup
description: Requirements, API keys or local runtimes, and IDE/project configuration so samples run out of the box.
meta-description: instructions how to setup your local development environment a smooth coding session
order: 10
---

= Technical Setup of your Development Environment for AI

Depending on which framework you use to access the AI, you will find instructions in its documentation on how to provide the API key to your application so it can connect to the API. Links to the documentation for the respective models are listed here:

* SpringAI (https://docs.spring.io/spring-ai/reference/api/chatmodel.html)
* LangChain4j (https://docs.langchain4j.dev/category/language-models)

== Local vs. Hosted LLMs (AI Large Language Models)

When integrating AI into your Vaadin application, you can choose between **hosted (cloud-based, commercial) LLMs** and **local (open-source, on-premise) LLMs**.
Both approaches let you connect your Java application to an AI model, but they differ in setup, cost, performance, and data privacy.

=== Hosted / Commercial LLMs
Hosted or commercial LLM providers (such as OpenAI or Anthropic) make their models available through an API.
You configure an *API key* in your application or development environment to authenticate requests.

Key characteristics:

* **Easy to integrate** – no local installation or GPU required.
* **Access to state-of-the-art models** – usually the latest, largest LLMs.
* **Usage-based pricing** – costs depend on tokens or requests.
* **Data leaves your system** – prompts and responses are processed by the provider.

[NOTE]
See the <<technical-setup,Technical Setup>> section for details on how to configure API keys in your local environment.

=== Local / Open-Source LLMs
Open-source models can be downloaded and run locally, giving you more control over privacy and performance.
Your Vaadin application communicates with the model through a local API endpoint (e.g., `http://localhost:1234`).

Key characteristics:

* **Full data privacy** – all prompts and responses stay on your machine.
* **No usage fees** – once downloaded, local models are free to run.
* **Hardware-dependent** – speed and quality depend on your CPU/GPU and RAM.
* **Model sizes vary** – choose smaller models for faster responses or larger ones for higher accuracy.

Popular tools:

* https://ollama.com/[Ollama] – run a variety of LLMs locally with a simple CLI and API.
* https://lmstudio.ai/[OpenLM Studio] – manage and run models locally with a graphical UI.

[TIP]
Local LLMs are great for development, prototyping, or offline use. Hosted LLMs are easier to scale in production and often provide better accuracy.

